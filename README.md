# Activation-Functions-NLP
Final Senior Intensive Project

Activation functions are often overlooked the tuning of hyperparameters in neural networks. Most deep learning researchers and engineers simply use the most popular activation functions, such as ReLU, without much consideration. In my senior capstone project, I investigated the performance of five different activation functions (ReLU, Swish, Mish, TAct, and mTAct) in many different neural network architectures in the context of image classification. In this project, I continue the investigation into the performance of activation functions -- but now in the context of neural network architectures designed for NLP. The task is simple sentiment analysis using a movie review dataset. 

In this repository, the code is available to run by downloading the Python module. To run the code, ensure that the proper dependencies are installed by uncommenting some or all of the cells at the top of the notebook to import special libraries. 
